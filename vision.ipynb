{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Подключаем и скачиваем библиотеки"
      ],
      "metadata": {
        "id": "GJnQXOwQ7W3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install numpy\n",
        "!pip install scipy\n",
        "!pip3 install idx2numpy\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense, Reshape, LSTM, BatchNormalization\n",
        "from keras.optimizers import SGD, RMSprop, Adam\n",
        "from keras import backend as K\n",
        "from keras.constraints import maxnorm\n",
        "import numpy as np\n",
        "from scipy.ndimage.measurements import center_of_mass\n",
        "import math \n",
        "import cv2\n",
        "import idx2numpy"
      ],
      "metadata": {
        "id": "n2xxXwfR3rSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Принимает целиком все изображение, находит в нем буквы и структуриет их по словам"
      ],
      "metadata": {
        "id": "JM0XHjdi7bUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow as n\n",
        "\n",
        "def vs(letters,height):\n",
        "    last = 0\n",
        "    a = []\n",
        "    for i in range(len(letters) - 1):\n",
        "        if letters[i+1][1] - letters[i][1] > height:\n",
        "            a += [letters[last:i+1]]\n",
        "            last = i + 1\n",
        "        if i == len(letters) - 2:\n",
        "            a += [letters[last:len(letters)]]\n",
        "    return a\n",
        "\n",
        "\n",
        "def vs_2(letters,weidth):\n",
        "    last = 0\n",
        "    a = []\n",
        "    for i in range(len(letters)-1):\n",
        "        if letters[i+1][0] - letters[i][0] > weidth*1.4:\n",
        "            a += [letters[last:i+1]]\n",
        "            last = i+1\n",
        "        if i == len(letters)-2:\n",
        "            a += [letters[last:len(letters)]]\n",
        "    return a\n",
        "\n",
        "def structure(letters):\n",
        "    height = 0\n",
        "    for i in letters:\n",
        "        height += i[3]\n",
        "    height = height/(2 *len(letters))\n",
        "\n",
        "    letters  = vs(letters,height)\n",
        "\n",
        "    for i in letters:\n",
        "        i.sort(key=lambda x: x[0], reverse=False)\n",
        "\n",
        "    for i in range(len(letters)):\n",
        "        weight = 0\n",
        "        for j in letters[i]:\n",
        "            weight += j[2]\n",
        "        weight = weight/len(letters[i])\n",
        "        \n",
        "\n",
        "        letters[i] = vs_2(letters[i],weight)\n",
        "\n",
        "\n",
        "    return(letters)\n",
        "\n",
        "def letters_extract(image_file: str, out_size=28):\n",
        "    img = cv2.imread(image_file)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    ret, thresh = cv2.threshold(gray, 130, 255, cv2.THRESH_BINARY)\n",
        "    img_erode = cv2.erode(thresh, np.ones((3, 3), np.uint8), iterations=1)\n",
        "\n",
        "    #Получаем контуры\n",
        "    contours, hierarchy = cv2.findContours(img_erode, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "    output = img.copy()\n",
        "\n",
        "    letters = []\n",
        "    for idx, contour in enumerate(contours):\n",
        "        (x, y, w, h) = cv2.boundingRect(contour)\n",
        "        if hierarchy[0][idx][3] == 0:\n",
        "            cv2.rectangle(output, (x, y), (x + w, y + h), (70, 0, 0), 1)\n",
        "            letter_crop = gray[y:y + h, x:x + w]\n",
        "            # print(letter_crop.shape)\n",
        "\n",
        "          \n",
        "            size_max = max(w, h)\n",
        "            letter_square = 255 * np.ones(shape=[size_max, size_max], dtype=np.uint8)\n",
        "            if w > h:\n",
        "                \n",
        "                y_pos = size_max//2 - h//2\n",
        "                letter_square[y_pos:y_pos + h, 0:w] = letter_crop\n",
        "            elif w < h:\n",
        "               \n",
        "                x_pos = size_max//2 - w//2\n",
        "                letter_square[0:h, x_pos:x_pos + w] = letter_crop\n",
        "            else:\n",
        "                letter_square = letter_crop\n",
        "\n",
        "            # Меняем размер изображения как в датасете\n",
        "            letters.append((x, y, w, h, cv2.resize(letter_square, (out_size, out_size), interpolation=cv2.INTER_AREA)))\n",
        "\n",
        "    # сортируем по координате y\n",
        "    letters.sort(key=lambda x: x[1], reverse=False)\n",
        "    \n",
        "\n",
        "    letters = structure(letters)\n",
        "\n",
        "    return letters\n",
        "\n",
        "letters = letters_extract('120.png')\n",
        "\n",
        "emnist_labels = [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122]\n",
        "\n"
      ],
      "metadata": {
        "id": "KzRn2_IrfY2p"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "считает веса сети (не запускать)"
      ],
      "metadata": {
        "id": "WYDQRIGA7rKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def emnist_model():\n",
        "    model = Sequential()\n",
        "    model.add(Convolution2D(filters=32, kernel_size=(3, 3), padding='valid', input_shape=(28, 28, 1), activation='relu'))\n",
        "    model.add(Convolution2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(len(emnist_labels), activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def emnist_train(model):\n",
        "    emnist_path = '/content/drive/MyDrive/'\n",
        "    X_train = idx2numpy.convert_from_file(emnist_path + 'emnist-byclass-train-images-idx3-ubyte')\n",
        "    y_train = idx2numpy.convert_from_file(emnist_path + 'emnist-byclass-train-labels-idx1-ubyte')\n",
        "\n",
        "    X_test = idx2numpy.convert_from_file(emnist_path + 'emnist-byclass-test-images-idx3-ubyte')\n",
        "    y_test = idx2numpy.convert_from_file(emnist_path + 'emnist-byclass-test-labels-idx1-ubyte')\n",
        "\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], 28, 28, 1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], 28, 28, 1))\n",
        "\n",
        "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, len(emnist_labels))\n",
        "\n",
        "    # тест\n",
        "    k = 10\n",
        "    X_train = X_train[:X_train.shape[0] // k]\n",
        "    y_train = y_train[:y_train.shape[0] // k]\n",
        "    X_test = X_test[:X_test.shape[0] // k]\n",
        "    y_test = y_test[:y_test.shape[0] // k]\n",
        "\n",
        "    X_train = X_train.astype(np.float32)\n",
        "    X_train /= 255.0\n",
        "    X_test = X_test.astype(np.float32)\n",
        "    X_test /= 255.0\n",
        "\n",
        "    x_train_cat = keras.utils.to_categorical(y_train, len(emnist_labels))\n",
        "    y_test_cat = keras.utils.to_categorical(y_test, len(emnist_labels))\n",
        "\n",
        "    \n",
        "    learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\n",
        "\n",
        "    model.fit(X_train, x_train_cat, validation_data=(X_test, y_test_cat), callbacks=[learning_rate_reduction], batch_size=64, epochs=30)\n",
        "\n",
        "\n",
        "\n",
        "model = emnist_model()\n",
        "emnist_train(model)\n",
        "model.save('emnist_letters.h5')\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkaOjm4rvhqJ",
        "outputId": "41e19d62-60c2-489b-d011-b54270280246"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(697932, 28, 28, 1) (697932,) (116323, 28, 28, 1) (116323,) 62\n",
            "Epoch 1/30\n",
            "1091/1091 [==============================] - 208s 190ms/step - loss: 4.0366 - accuracy: 0.0735 - val_loss: 3.9108 - val_accuracy: 0.1837 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "1091/1091 [==============================] - 206s 188ms/step - loss: 3.7903 - accuracy: 0.1468 - val_loss: 3.5971 - val_accuracy: 0.2718 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "1091/1091 [==============================] - 209s 192ms/step - loss: 3.4960 - accuracy: 0.2030 - val_loss: 3.2751 - val_accuracy: 0.3643 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "1091/1091 [==============================] - 209s 192ms/step - loss: 3.2380 - accuracy: 0.2704 - val_loss: 2.9808 - val_accuracy: 0.4091 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "1091/1091 [==============================] - 202s 185ms/step - loss: 2.9946 - accuracy: 0.3236 - val_loss: 2.7109 - val_accuracy: 0.4333 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "1091/1091 [==============================] - 205s 188ms/step - loss: 2.7746 - accuracy: 0.3634 - val_loss: 2.4827 - val_accuracy: 0.4535 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "1091/1091 [==============================] - 201s 185ms/step - loss: 2.5896 - accuracy: 0.3973 - val_loss: 2.3003 - val_accuracy: 0.4795 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "1091/1091 [==============================] - 202s 185ms/step - loss: 2.4449 - accuracy: 0.4213 - val_loss: 2.1565 - val_accuracy: 0.5004 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "1091/1091 [==============================] - 199s 182ms/step - loss: 2.3260 - accuracy: 0.4431 - val_loss: 2.0418 - val_accuracy: 0.5183 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "1091/1091 [==============================] - 198s 181ms/step - loss: 2.2275 - accuracy: 0.4583 - val_loss: 1.9465 - val_accuracy: 0.5340 - lr: 0.0010\n",
            "Epoch 11/30\n",
            "1091/1091 [==============================] - 202s 185ms/step - loss: 2.1473 - accuracy: 0.4747 - val_loss: 1.8666 - val_accuracy: 0.5463 - lr: 0.0010\n",
            "Epoch 12/30\n",
            "1091/1091 [==============================] - 200s 183ms/step - loss: 2.0772 - accuracy: 0.4858 - val_loss: 1.7989 - val_accuracy: 0.5597 - lr: 0.0010\n",
            "Epoch 13/30\n",
            "1091/1091 [==============================] - 203s 186ms/step - loss: 2.0170 - accuracy: 0.4979 - val_loss: 1.7403 - val_accuracy: 0.5690 - lr: 0.0010\n",
            "Epoch 14/30\n",
            "1091/1091 [==============================] - 203s 186ms/step - loss: 1.9680 - accuracy: 0.5061 - val_loss: 1.6905 - val_accuracy: 0.5769 - lr: 0.0010\n",
            "Epoch 15/30\n",
            "1091/1091 [==============================] - 205s 188ms/step - loss: 1.9156 - accuracy: 0.5160 - val_loss: 1.6446 - val_accuracy: 0.5863 - lr: 0.0010\n",
            "Epoch 16/30\n",
            "1091/1091 [==============================] - 204s 187ms/step - loss: 1.8819 - accuracy: 0.5223 - val_loss: 1.6047 - val_accuracy: 0.5936 - lr: 0.0010\n",
            "Epoch 17/30\n",
            "1091/1091 [==============================] - 205s 188ms/step - loss: 1.8346 - accuracy: 0.5326 - val_loss: 1.5684 - val_accuracy: 0.5996 - lr: 0.0010\n",
            "Epoch 18/30\n",
            "1091/1091 [==============================] - 207s 189ms/step - loss: 1.8077 - accuracy: 0.5364 - val_loss: 1.5356 - val_accuracy: 0.6046 - lr: 0.0010\n",
            "Epoch 19/30\n",
            "1091/1091 [==============================] - 206s 189ms/step - loss: 1.7760 - accuracy: 0.5427 - val_loss: 1.5047 - val_accuracy: 0.6100 - lr: 0.0010\n",
            "Epoch 20/30\n",
            "1091/1091 [==============================] - 205s 188ms/step - loss: 1.7502 - accuracy: 0.5494 - val_loss: 1.4773 - val_accuracy: 0.6153 - lr: 0.0010\n",
            "Epoch 21/30\n",
            "1091/1091 [==============================] - 207s 190ms/step - loss: 1.7242 - accuracy: 0.5536 - val_loss: 1.4524 - val_accuracy: 0.6176 - lr: 0.0010\n",
            "Epoch 22/30\n",
            "1091/1091 [==============================] - 206s 189ms/step - loss: 1.6964 - accuracy: 0.5573 - val_loss: 1.4274 - val_accuracy: 0.6231 - lr: 0.0010\n",
            "Epoch 23/30\n",
            "1091/1091 [==============================] - 204s 187ms/step - loss: 1.6787 - accuracy: 0.5617 - val_loss: 1.4069 - val_accuracy: 0.6268 - lr: 0.0010\n",
            "Epoch 24/30\n",
            "1091/1091 [==============================] - 208s 191ms/step - loss: 1.6538 - accuracy: 0.5658 - val_loss: 1.3852 - val_accuracy: 0.6317 - lr: 0.0010\n",
            "Epoch 25/30\n",
            "1091/1091 [==============================] - 207s 189ms/step - loss: 1.6362 - accuracy: 0.5696 - val_loss: 1.3673 - val_accuracy: 0.6357 - lr: 0.0010\n",
            "Epoch 26/30\n",
            "1091/1091 [==============================] - 209s 191ms/step - loss: 1.6185 - accuracy: 0.5732 - val_loss: 1.3487 - val_accuracy: 0.6388 - lr: 0.0010\n",
            "Epoch 27/30\n",
            "1091/1091 [==============================] - 206s 189ms/step - loss: 1.5986 - accuracy: 0.5777 - val_loss: 1.3331 - val_accuracy: 0.6424 - lr: 0.0010\n",
            "Epoch 28/30\n",
            "1091/1091 [==============================] - 206s 189ms/step - loss: 1.5869 - accuracy: 0.5796 - val_loss: 1.3165 - val_accuracy: 0.6445 - lr: 0.0010\n",
            "Epoch 29/30\n",
            "1091/1091 [==============================] - 207s 190ms/step - loss: 1.5724 - accuracy: 0.5838 - val_loss: 1.3024 - val_accuracy: 0.6474 - lr: 0.0010\n",
            "Epoch 30/30\n",
            "1091/1091 [==============================] - 206s 189ms/step - loss: 1.5583 - accuracy: 0.5848 - val_loss: 1.2889 - val_accuracy: 0.6515 - lr: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем и считаем готовую модель, после чего сохраняем ее"
      ],
      "metadata": {
        "id": "wAEhW8z48BjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция которая используя предыдущие преобразование и нахождения букв, преобразует фотографию каждого символа в сам символ"
      ],
      "metadata": {
        "id": "1_JOqo8O8K7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def img_to_str(model, image_file: str):\n",
        "    letters = letters_extract(image_file)\n",
        "    s_out = \"\"\n",
        "    for j in letters:\n",
        "        for lett in j:    \n",
        "            for i in range(len(lett)):\n",
        "                s_out += emnist_predict_img(model, lett[i][4])\n",
        "            s_out += ' '\n",
        "        s_out += '\\n'\n",
        "    return s_out\n",
        "\n"
      ],
      "metadata": {
        "id": "RnrDWEw07D5u"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем модель, после чего запускаем алгоритм распознавания"
      ],
      "metadata": {
        "id": "5BouLf4-8wsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def emnist_predict_img(model, img):\n",
        "    n(img)\n",
        "\n",
        "    img_arr = np.expand_dims(img, axis=0)\n",
        "    img_arr = 1 - img_arr/255.0\n",
        "    img_arr[0] = np.rot90(img_arr[0], 3)\n",
        "    img_arr[0] = np.fliplr(img_arr[0])\n",
        "    img_arr = img_arr.reshape((1, 28, 28, 1))\n",
        "    \n",
        "\n",
        "\n",
        "    predict = model.predict([img_arr])\n",
        "    result = np.argmax(predict, axis=1)\n",
        "    return chr(emnist_labels[result[0]])\n",
        "\n",
        "\n",
        "model = keras.models.load_model('emnist_letters.h5')\n",
        "s_out = img_to_str(model, \"120.png\")\n",
        "print('\\n\\nresult: ',s_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "id": "vXAxEmKw84nz",
        "outputId": "11dd75e3-9ab6-446b-f5bb-205229a9ebaa"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F0064B51850>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAklEQVR4nM3PPUsDMRgH8H9y8a5S9UBOpBxIQcG6dOhsB8HF1dUv51coOHTSvThZB/EFRBSxiKU9veOSa9Lh+kRSqnOfJeH55eX/MAOon1T7GxXMaiLHudisABAABr3rpH5yQDi8u3rZPmuwEkf9i89Wy2Jy373ZO24A4ADMREmlyaCVlMpghnNl7G4BYunQxhXlUgxvfWq9PX27mD6edwizr/e5m+MHQagLyR3kq3FImI8GysEgPm0SfvQvXx30o+Yh4XPWc//0qluxDRRS8EVzMvyDWAJkfyLzuCd+n2BgvDwsAKzvHyW7kcVqvb1TiwCAGaDIMr2yFhAWeapEGACYAlaCUABAvID+AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 77ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F0064B51670>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACbUlEQVR4nFXSzVPTUBQF8HNfXtMkpU1aoB8MVBRRtIoj6ALR8b92xpXjwmEDKOAgMgMOtJSYtilt2nw0yXsuAMWzvL/FmTlzSeImIvXcgacUp02N6PrEAQBSRCM/cO3elVKpzpi6Zqj/MA3bX0977tAP2JRZKC8sPi3TDUo5tvc/7NgeEUlBmWpjTdf1zDWKuPl55/vvQDUNNR35Ue8wNqJHVQIHELg/Pu314vzMnKUlV53uoOmryFmqwgFpb3056rGl9UatwEV4eXy41z9QSvlankOK7u5OO6023q/P5kgKe177fXJpLC+aUxxy0j9rjmtv3q7OGxkAlEkukvaw1VqUHGkw6PSTwsqKlY4IgMxX5y/c0G774Ji4bihZ2j/rZm+2armTNO50I8kRdZxQUvArptshB+fDOOo6oeSYdHqRlP1vJ7eGaDgKU8+LwZF4XgIR2L2/KIXIEicCBxiB1NLDIu6GsecW41A0TSFmNRaJ7iCxukUcaqmkIjUevcwo/2nBIo5suazJlNdWTO2mcjKeCEXNZokjUyrpTIxO68saAUCaNLfOo9y95fsljoxZMo3x8MdMTlcVhiTwjj7u++UNvVLkIF580D692o2jcb2YTYbnP7cPW6lVrOjgIGY9dgb2WcLFuGrE3aPdgws/Z9YqGnGAKhtKQI69fblV0FLPuXSGuSdrS9NZ4gBZz8gR5LSaYComsVT0+qvX9TwDB8DUuXdW4bh35SchBKmzy43NZ9N0/WCE2Y05Y7p54QZCMjW3tPlitUYEkLyeetB0hu5g7Es1ly8vVGengFuEFEkqQm/sCT2f11WuMAD4A39aKnCTVzXbAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F006C045EE0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6UlEQVR4nO3Rz0rDQBDH8e8mm9paiYWiRbB6EZFcvFl9AJ+hD9an8Qkq3utBxEOo1n8xwUBDdt2sB5UsDXj24MBc5sMMPxhhAT6S+YPBaw2GmwF1SQDMYnqp8TrD49OtoIHZzbS01viH3ZOeWEG/dzDS+u36Rc72Q38Vd872jIqXhbpPqwb2wyPs3exRlboRSEgJhG0f6xgev9Q//hG07s/kz/C7v1i4qG6flGUxL1R8lUvWBrshgLAAyeTi1aKzpe1utAXb5+Oo3jRJ/GwAyHMQZaqds60ozZwk/SikPlu9F5WT0u+sBwCf8nVXpBpeY5AAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F006C1269D0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5klEQVR4nO3SMUsDQRCG4XfuPDgPTUxEU0iUVCnsbCSVZao0Vv4Tf5KNla2tpVhFiIJIEg0IkQVPzV1y7lgkkIUF6xRO+zDfDMyIAujoPrNIqVZPQpa1BoAOrkyBVBvHzZ3Aw/Ht20wLqTycnXgo20emmPRH/bD1E3m438nsx91Nd2jUmym1LSXdfe/mhY9EEUgpRl0j4I/6x1XExT1RmJ9SFRAX0+GLZfL0qvbxOoawuRcCiAL0Li5zbJ5+UikHsH5+miw7p+PnbJ5kDLCRqhO7edieOovEB4u/U4DZ17f7WVJOBOAXNTRRsmkWrtUAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 28ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F0066CA30D0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACb0lEQVR4nE3S2U8TURQG8HPuvTOdaWdraxdouijgghgTEKLBBx/8vwkmbokCiYLVxEAFWrrPdHo709muDy3oeTy/p/N9BwUsRgTewPbjXM5SyXIFDABAxCHn00l7wONSqWiqqpLCO4x57+xsYDs8TDQ9m6uvNWq3KBKvfXpw2JkkgJhAOr+1F5gZCRc463w8/NXzBJNkOvODcTNIpPWyBMAAwm7zw4Hjq5aeUaTxxJ1f2STFNJ0iAzH99O54FGnbW7W8jO7gS7PDf0SK2shSJqLJ6efLUK/t7m8UmPD6ChXn3flaNWtQlkz7Nz2uPN1/tVlIE6GmXmclx561zxsVYLHTH/Mw9+TNoxUJASVj02w3Qz5o2TGQxOnxhKrl+xZDAACS0kpVSzi9aQIkHnU4yHqxYixTo2qpnk3GHTcGFttdjlo+Q/E2UaoXMmI6dENBEm77mLGUOwOiGorwXD8SBBARCbtrAgBACABABIJymkESi/8o9COQFIkioea9tPAc/5/G7pCjlsswIDS3qgEfOTxccjIfXk+IVTYYEmqVMuDbrZ/DeGG+074YE7OoEWDELBryzPl9TOQMQ4jn/YvzS4fkKwYBRrTCSjnyzsR0slaQhN39enI0DvXVjTwFRhSzvh5ctX0p9KqppPfn/cl1oJbrDYsCA9T25JTnDo87RyYV3Gl1Z8rWyxc1HYEBKJvGdHh502kvrkSaXtl5+7AoIzAAohR28Nv3vuMFAqhk5quPd9dzywdDKbtdKeutm9EsRjm9Wn32/EGNIgCgAAAR8fa1Mxm7QWKYllksWQbCLYIQUZSEtuvH+azMKCOLHv4CJWMq6EiXmGUAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F0064B51DF0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACU0lEQVR4nH3Ly27TQBSA4XNmxh67sTOTxNdcWpWigriJBX0H3pUFz8ADULECIVGJpmmbOG58SeLEl3hYtCzh3376UcG/Y7uiVjrnGii139TAOdcQ2npdommx2Y+7wp08H2GzvvoWQxCeT7TDdnZ5Q84/sJsvl/HZRSekVXz56Re8fMsDrUl/fv5KPoYMcT2DfqpUufh9fQ2EXRygWc1uZnyrESsUkMcFQBktS4B8nh2gSeM9PeoKYg8lyZfbVpWLqAIo4mzX1ElUmu6AE2sosUi2dbuPohJJmad51azi0gwGGrFCSZt9nlVVtKyoTpt0samTZWn6A0aOPGmQKo03Rbw6mIMOJnfrerUszdBhhPJu31TZ4iHNSxa+GdL0flNlD5Xh9xmjmvSjJrklyzXYJ+9UtJo9iDRVndDRCKDtWyqf3y72pj85FpjdR2nRoOlIygDkSK5W0zLddcZnZ99JMp0etmjKvkUZoO3bKp8fNqXpjSeCZIs7VmiyZ1BkgHIk2mS2qYpgfOz2ullyVaS65xsIDMAObJWrbVuagWuJ/rK8zhLdGegADNDyhL5Od1hboaPL4f1inqyFH3AAAmCKrqF2Wa4sR9Bu2FVJVOiu+4hU73lm26AYWDqKscCmarnnPSJQGVgA3HUNgmIoKADhjqM/IpGBhcg9zwAQY0GAGN2ezQAYANBeaKHiga8D2IHUkHZk18S/Z2ghGp7LAUwpbV0f+AbC09l/9ipvT16EHEC3T98H9PS1BABABdAW6TRS9ti3NIDm6qZA2x3JJ/xn5D8GfwDiJgyhcdHlzQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F0066CA3B20>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACgklEQVR4nD3RWW/bVhAF4DNzL0lJFEUq2qLITuKgLZIgDuqi6QIESFEU/d9FAwTZ4MJ1vciJ6toRFLEWN4kUZZH39sF253E+nHk4QxpXo1UWBaGwvVZV8vVOahCg8+VyGU99XzqdO06tbhtMAKTWAtCTk9F4Fs9TNm3P6zza7lQEAAkNfbk4+u3dcbjSrDWEO3hBj25XiSCJUUwOXr4+Tdao2MWqUNnk/TJ8ftsiSMI6Gb189WdqNOquV6RpmsWHE9EVPQsSKhi+fTVcNDa+3OnZenXx99Ef6XrXin7uQEKHJ4fDGfe3n/3Us6icHtmz0/SUnGctllDRx0+Z1dz59fFmXZCSBtav9+Pz8XxlSaj43C/cjSffbxoMcN3UKhnNw1mYmQyd+WHp3u/XDQYAks2tvqnL7N+oZF3Mp5FqPuhXrkvjxt1+TSL7fFFwmSTZSttdV1wXSlypuzXO/LBklS/zAhXPvkEIadUsuowXiolIa7AUdIMgrRWICEymIQVUUd78DrosLkstqhYxVx2nwqskUzdW5Oki19W2J5hrjVt1no9nl9dRnQdBkmu735YMqnWbNN0bpepKVXB4tgJXO55gsDNoy4u/jsdBXmhV5NE/ux+Xsuq2XJag1lefh/Fst/b0adcpMn+0+/vx0r230TBJgt0H5518+UHFuNtcz8/29/ZD6j3cspkk2P0inb85CE+SM69armLfj6xb3/yy3SBIkD0oVRFPF8kQpAGwOdj69sdNC5AAGe3HKU7GQbRWINN2u092vm6bBEgA7DlGa+/g0yRdQ9it/v3vfuhZDFwdgo4C/yKO4oQrTtN17wxspv8RqizLdR4Hwq43DCEkEQD8B/mzOu5A3i1CAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F0064BFFFA0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABtklEQVR4nI3Q32sTQRAH8O/szd6P1rumSduYpm08Wwr+AkGkD6L4V4sgivjoS6tVaLFQ6EM0qe0l6Xl3ye3t+nAx5oSA8zD748MyO0MGxa9UaZC0l2yCVldRNDTk+e11ZkD1LpMJhO3VVzx7GHV7/etC1JpenRlIPh/2bwyx1+mEGx8/9UZxYvh2uBWCgezk/flPrY27d//J7qs3AwWQ3VEHGgxY/lqMLMvVD5OefUs0CYtsKS2AAdnc89bjQX94FV/XblzfcSTZm61bAmSQXfTieHDx9igVjr/Z3m/7Lqzl2t2WZMAJt3UenZ5+gVFm6+DpbuCCyJIWGCApUaDuEoTXfPzyzqpjEQAAXC6QDhN4ZefeI58wtRmCAMi1sG4LzGJuC8jmfmP+ooK8uh0sRMvfWKJFSGzzQgRVj9WXRLQQ/43/Q1NmM4/8F0oyBsBstjofJVqPu7FGMeoWiSBw4FOJk+/vvmamGB6PER+myx6RaDx/IUtUlx9exzBGFUhPzohAvNN4NkXCJE2mNZUCQHKi/tQUTqMdVxqQraD8EZk8OjrP55FE8PABl2j0OK90RxC2AwD4DToto6v8EvY8AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F006526A400>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5ElEQVR4nO2Sv0sCcRiHn1dFwfMucDBqqIYKWmpraG0RGvsTWvu32ttaWoOmlIK2UI9SCCk8/ErX9+7ehgS/3IGzg8/68P6Az0cUgPGzSRGvteOXWVABQD/vRhYJdk+Ptkt5yaTTizWl8Xp5USvI4GTz14bvH4/HSUFKq23Sn+7D03CcFW7SPMuIt8yLsaqSl2UPqkEdxaXEEtZyFeU8T5T/LFUVEFeaYWix/V7K4L4usL9XARAFeLu9idDYRNlGUwSur/zFpP0OvwCQKAJk4q71Ds+n7icH81IqQDIzbiXxfQH4A5gRT7GFpsEwAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7F0064BFA400>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABk0lEQVR4nJ3SzUsCQRgG8Hdc1y13NU3LwkoJIsqCiKBDUdQhuvS3BhFGFkFGdemWWgpWam0b5cduq7vOujMdzHC2Tr3H+fHOPDwMouAYq5rTAgshBABup1FLTj7HQkEOAFxOhM7HdepKIfAnUkurq5j+jX3zf+xLaxuayUte+hsp1vXX0rs0GfVg4kRSOM6+aCYvhZcGsQOt/Ol+XnUNEuwujzVY7NRSBxkyEgrbtWoha7DYuDnPdBIba8O0epHOsm9S9bbYHNveW/KCHiZ1GTObelHzze2sugGk1VpZ7WG3hFalGVmJuAEA8dPLPqYh2lRa0pQEAAAoOCEwCG0V8xLfPRFEjkUXh6j93Rv9CdtFNDAs4F4MQ+2wm2JU1J40CgBAPsomi94pSbkstWxKiXF3qbLX+mcDrYfDlIzNytFJwVGff3H+rX6mK+NETmdMF2LQl9htnynHFzy1jUBMVhnkhtYp5ORXgoToZjx5j/oRuDjiZx7fTU8gvjUJ8eBoN0vvy1im+VlriBFR5HXM+QUEAF//YLZXpv1/ewAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n",
            "\n",
            "\n",
            "result:  nB110 n051d \n",
            "\n"
          ]
        }
      ]
    }
  ]
}